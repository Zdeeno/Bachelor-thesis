\chapter{RL basics}
At first we have to define an environment where agent can operate. Environment can be described as Markov decision process, where $S_t \in \mathcal{S}$ is state from set of possible states $\mathcal{S}$ in which is environment located in time $t$. Agent can observe environment's state and take action accordingly. Action is a transition between states. Every action $A_t \in \mathcal{A}$ moves environment from $S_t$ to $S_{t+1}$. Environment evaluates every action and return appropriate reward $R_t$. In RL is set $\mathcal{A}$ often called action space and set $\mathcal{S}$ observation space. Agent's main goal is to maximise reward.

\begin{figure}[!h]
\includegraphics[scale=0.3]{images/RL-concept.png}
\caption{RL concept}
\end{figure}

Major issue is that maximising immediate reward often isn't good approach to maximise overall reward. This greedy policy can take us into very disadvantageous states. Thus agent must take in account future states and rewards. In past agents used to contain big tables which stored information about quality of every action in every state. That is possible in environments with small action and observation spaces, but very memory consuming for larger environments and even impossible for continuous action or observation space. Therefore modern methods use neural networks as function approximators.

\section{Temporal difference learning}
Temporal difference (TD) learning is combining ideas of Monte Carlo methods and dynamic programming. It is able to learn directly from experience obtained by interactions with environment without any knowledge about environment. TD learning is done by following assignment in each timestamp [Sutton]
\begin{equation}
V(S_t) \gets V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\end{equation}
where $V$ is so called state value, which tells us how good is being in particular state with current policy. $\alpha \in \mathbb{R}^+$ is step size and $\gamma \in (0, 1)$ is discount factor.

\section{Q-learning}
Q-learning is type of TD learning developed by Watkins [1989]. The state value $V$ from previous section is replaced by $Q$ value, which refers to quality of action in particular state instead of quality of state itself. When we rewrite TD learning (1.1) to Q-learning we get:
\begin{equation}
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha [R_{t+1} + \gamma max_{A_{t+1}} Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)].
\end{equation}
Our policy here is to take action with maximal $Q$ value. That is called greedy policy. Obvious drawback of greedy policy is that we won't explore whole environment properly, because in some states we would always take same action with highest $Q$ value. Solution to this problem is sometimes take random action to explore the environment. This policy is often referred as $\epsilon$-greedy policy.

\begin{algorithm}
\caption{$\epsilon$-greedy policy}\label{euclid}
\begin{algorithmic}[1]
\Procedure{ChooseAction}{}
\State $\epsilon \gets \epsilon \cdot \epsilon_d$
\If {$\epsilon >$ random $\in (0,1)$}
\State action $\gets$ random $\in \mathcal{A}$
\Else 
\State action $\gets$ $argmax_{A_t} Q(S_t, A_t)$
\EndIf
\State \Return action
\EndProcedure
\end{algorithmic}
\end{algorithm}

It is common to set $\epsilon = 1$ at the beginning of the training and decay rate $\epsilon_d$ close to one. This policy assumes that at first you need to explore environment and then exploit agent's experience.

\chapter{Deep neural networks in RL}
As we stated in previous chapter, tabular methods are very inefficient when it comes to large environments. Here comes in play deep neural networks which can replace tables. Deep Q networks (DQN) proposed by Google's Deepmind [2015] outperformed all previous RL algorithms in playing Atari games. With neural networks grew also popularity of policy gradient methods [Sutton], where neural network outputs specific action instead of Q values.

\section{DQN}
Neural network takes current state as input and outputs Q value for each possible action. Network is trained using gradient descent. As loss function $L$ is commonly used mean squared error between currently predicted Q value and discounted maximal future Q value plus reward.
\begin{equation}
L = \frac{1}{2}[R_{t+1} + \gamma max_{A_{t+1}}Q(S_{t+1}, A_{t+1};\theta) - Q(S_t, A_t;\theta)]^2
\end{equation}
where $\theta$ is set of weights of our function estimator (neural network). Unfortunately this simple DQN agent suffers from lack of sample efficiency and does not converge well. There is a lot of techniques which can help DQNs to achieve good results.

\subsection{Experience replay}
Experience replay is biologically inspired mechanism which stores all experiences (specifically: $S_t$, $A_t$, $R_{t+1}$, $S_{t+1}$) into buffer and then randomly choose batch of samples for training. It removes correlation in the observation sequence and improves sample efficiency of DQN.

\subsection{Target network}
Target network is another technique proposed by Mnih[Citace] to improve convergence of DQN learning. It uses two neural nets instead of one. We train first - training network on batch of data, but we use second - target network for predictions during training. When is training on batch completed, we update target network.
\begin{equation}
\theta^- = \tau \theta + (1-\tau)\theta^-
\end{equation}
where $\theta^-$ is set of trainable weights of target network, $\theta$ indicates online network weights and $\tau << 1$ is constant.
Loss function is now computed using both target and online network.
\begin{equation}
L = \frac{1}{2}[R_{t+1} + \gamma max_{A_{t+1}}Q(S_{t+1}, A_{t+1};\theta^-) - Q(S_t, A_t;\theta)]^2
\end{equation}
Target network stabilize training since predicting network isn't changing after every training step.

\subsection{Double Q-learning}