@book{sutton2012, place={Cambridge, MA}, title={Reinforcement learning: an introduction}, volume={2}, url={http://incompleteideas.net/book/bookdraft2017nov5.pdf}, publisher={The MIT Press}, author={Sutton, Richard S. and Barto, Andrew G.}, year={2012}}

@article{mnih2015,
title={Human-level control through deep reinforcement learning},
volume={518},
DOI={10.1038/nature14236},
number={7540},
journal={Nature},
author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg et al.},
year={2015},
pages={529-533}
}

@article{plappert2017,
   author = {{Plappert}, M. and {Houthooft}, R. and {Dhariwal}, P. and {Sidor}, S. and 
	{Chen}, R.~Y. and {Chen}, X. and {Asfour}, T. and {Abbeel}, P. and 
	{Andrychowicz}, M.},
    title = "{Parameter Space Noise for Exploration}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.01905},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Statistics - Machine Learning},
     year = {2017},
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170601905P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{silver2014,
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
year = {2014},
month = {06},
pages = {},
title = {Deterministic Policy Gradient Algorithms},
volume = {1},
booktitle = {31st International Conference on Machine Learning, ICML 2014}
}

@article{zimmermann2017,
   author = {{Zimmermann}, K. and {Petricek}, T. and {Salansky}, V. and {Svoboda}, T.
	},
    title = "{Learning for Active 3D Mapping}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1708.02074},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = {2017},
    month = aug,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170802074Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{sutton1999,
 author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
 series = {NIPS'99},
 year = {1999},
 location = {Denver, CO},
 pages = {1057--1063},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3009657.3009806},
 acmid = {3009806},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@article{geiger2013,
  author = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
  title = {Vision meets Robotics: The KITTI Dataset},
  journal = {International Journal of Robotics Research (IJRR)},
  year = {2013}
}

@misc{rozsypalek2018,
  author = {Rozsypálek, Zdeněk},
  title = {Lidar-gym, training environment in openAI interface},
  year = {2018},
  publisher = {GitLab, CVUT FEL},
  journal = {GitLab repository},
  howpublished = {\url{https://gitlab.fel.cvut.cz/rozsyzde/lidar-gym}},
}

@misc{quanergy2016,
  author = {Ackerman, Evan},
  title = {Quanergy Announces \$250 Solid-State LIDAR for Cars, Robots, and More},
  year = {2016},
  publisher = {spectrum-ieee},
  howpublished = {\url{https://spectrum.ieee.org/cars-that-think/transportation/sensors/quanergy-solid-state-lidar}},
}

@article{schaul2015,
   author = {{Schaul}, T. and {Quan}, J. and {Antonoglou}, I. and {Silver}, D.
	},
    title = "{Prioritized Experience Replay}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.05952},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = {2015},
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151105952S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{hasselt2015,
   author = {{van Hasselt}, H. and {Guez}, A. and {Silver}, D.},
    title = "{Deep Reinforcement Learning with Double Q-learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1509.06461},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = {2015},
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150906461V},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{watkins1992,
    author = {Christopher J. C. H. Watkins and Peter Dayan},
    title = {Q-learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {279--292}
}

@article{lilicrap2015,
   author = {{Lillicrap}, T.~P. and {Hunt}, J.~J. and {Pritzel}, A. and {Heess}, N. and 
	{Erez}, T. and {Tassa}, Y. and {Silver}, D. and {Wierstra}, D.
	},
    title = "{Continuous control with deep reinforcement learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1509.02971},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = {2015},
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150902971L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{openai2016,
   author = {{Brockman}, G. and {Cheung}, V. and {Pettersson}, L. and {Schneider}, J. and 
	{Schulman}, J. and {Tang}, J. and {Zaremba}, W.},
    title = "{OpenAI Gym}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.01540},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
     year = {2016},
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160601540B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{tensorflow2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@misc{keras2015,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{petricek2017,
  author = {Petříček, Tomáš},
  title = {Voxel Map, simple C++ header-only library with Matlab and Python interfaces for dealing with 3-D voxel maps},
  year = {2017},
  publisher = {Bitbucket.org},
  howpublished = {\url{https://bitbucket.org/tpetricek/voxel_map}},
}

@article{mayavi2011,
  title={{Mayavi: 3D Visualization of Scientific Data}},
  author={Ramachandran, P. and Varoquaux, G.},
  journal={Computing in Science \& Engineering},
  volume={13},
  number={2},
  pages={40--51},
  issn={1521-9615},
  year={2011},
  publisher={IEEE}
}

@misc{openai2017,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@inproceedings{matconvnet2015,
      author    = {A. Vedaldi and K. Lenc},
      title     = {MatConvNet -- Convolutional Neural Networks for MATLAB},
      booktitle = {Proceeding of the {ACM} Int. Conf. on Multimedia},
      year      = {2015},
}

@article{schulman2017,
   author = {{Schulman}, J. and {Wolski}, F. and {Dhariwal}, P. and {Radford}, A. and 
	{Klimov}, O.},
    title = "{Proximal Policy Optimization Algorithms}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1707.06347},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170706347S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{hess2015,
   author = {{Heess}, N. and {Wayne}, G. and {Silver}, D. and {Lillicrap}, T. and 
	{Tassa}, Y. and {Erez}, T.},
    title = "{Learning Continuous Control Policies by Stochastic Value Gradients}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1510.09142},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2015,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151009142H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@InProceedings{chou17,
  title = 	 {Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution},
  author = 	 {Po-Wei Chou and Daniel Maturana and Sebastian Scherer},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {834--843},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/chou17a/chou17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/chou17a.html},
  abstract = 	 {Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic policy. In this work, we propose to use the Beta distribution as an alternative and analyze the bias and variance of the policy gradients of both policies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on- and off-policy stochastic methods respectively, on OpenAI Gym’s and MuJoCo’s continuous control environments.}
}

@ARTICLE{adam2014,
   author = {{Kingma}, D.~P. and {Ba}, J.},
    title = "{Adam: A Method for Stochastic Optimization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1412.6980},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2014,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
