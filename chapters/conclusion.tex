\section{Conclusion}
First, we overviewed reinforcement learning concepts and described several methods which help convergence of the learning process. Then, we addressed the challenging multi-dimensional control task of selecting depth-measuring rays for 3D mapping. Various agents and model architectures were implemented and compared. All deterministic agents performed poorly in this specific task. The stochastic agent successfully outperformed the random planner. Action space size and time-complexity were two major blockers during the training. None of the trained RL agents can compete with the prioritized greedy planner proposed by Zimmermann et al. \cite{zimmermann2017}.


\subsection{Future work}
Experiment which would be worth conducting is an agents, which stands between simple and extended stochastic agent. The extended stochastic agent has action space consisting of 60 real numbers (15 rays with azimuth and elevation and for each alpha, beta parameters). That is very likely too much for network architecture used by agent. On the other hand, when only one distribution is outputted for all rays, it does not allow agent to create advanced policies. Solution could be to output for example three different distributions, each describing five rays. Another improvement could be achieved by adjusting neural network architecture. Especially splitting and merging layers of neural network, can have significant impact on performance. 
