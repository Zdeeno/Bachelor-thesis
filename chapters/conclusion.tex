\section{Conclusion}
First, we overviewed reinforcement learning concepts and described several methods which help convergence of the learning process. Then, we addressed the challenging multi-dimensional control task of selecting depth-measuring rays for the 3D mapping. Various agents and model architectures were implemented and compared. All deterministic agents performed poorly in this specific task. The stochastic agent successfully outperformed the random planner. Action space size and time-complexity were two major blockers during the training. None of the trained RL agents can compete with the prioritized greedy planner proposed by Zimmermann et al. \cite{zimmermann2017}.


\subsection{Future work}
We propose further experiments with an agent, which stands between the simple and the extended stochastic agent. The extended stochastic agent has the action space consisting of 60 real numbers (15 rays with azimuth and elevation and for each alpha, beta parameters). That is very likely too much for the network architecture used in the experiments. On the other hand, when only one distribution is outputted for all rays, it does not allow the agent to create an advanced policies, because the Beta distribution considered in this thesis is always unimodal. A solution could be to output for example three different distributions, each describing five rays. That would allow agent to output a density function with multiple local maxima. \par Another improvement could be achieved by adjusting the neural network architecture. Especially splitting the network into two subnetworks before the output or different types of merging the input layers can have a significant impact on performance. Finally, the reinforcement learning agent can be almost always improved by a better reward function, but we find very difficult to improve the existing reward function.
