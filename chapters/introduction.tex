\section{Introduction}

Lidar sensors offer an accurate distance measurements, which can be used for mapping srounding space. There is much utilization of volumetric space reconstructions in different fields. For example, lidar sensors are nowadays essential equipment for a large variety of autonomous vehicles. The sensor can help autonomous vehicles to orient itself in an environment. One of the most significant issues which prevent broader implementation of these sensors is a relatively high price. Breakthrough in this field is solid-state lidar. These lidars do not have moving parts, and their price should be circa hundreds of dollars \cite{quanergy2016}. Solid-state lidar can send limited number of rays in chosen directions per timestamp. Zimmermann et al. \cite{zimmermann2017} proposed mapping agent which creates dense reconstructions from sparse measurements. They also proposed prioritized greedy planning for choosing directions of these rays. \par The objective of this thesis is to apply reinforcement learning (RL) methods to learn planning of the rays and contribute to methods of controlling these sensors. RL is a field of study based on concepts of behavioral psychology, especially the trial and error method, and has in recent years experienced a rapid development due to the growth of computational power and neural networks improvement. Richard Sutton has made a helpful summary of RL concepts in his book \cite{sutton2012}. One of the biggest achievements was playing Atari games by a RL agent without any prior knowledge of the environment \cite{mnih2015}. Soon after was introduced a RL agent, able to solve simple continuous problems such as balancing inverse pendulum on a cart. Today state-of-the-art methods can solve complex problems with infinite action spaces. Although these methods reach great success, they still suffer from lack of sample efficiency - they need for training a lot of interactions with the environment. This inefficiency makes creating an agent controlling lidar very challenging, since training large neural networks is very time-consuming. \par The agent is divided into two parts - mapping and planning. The mapping part should create the best possible reconstruction from sparse measurements, while the planning part is focused on picking rays that will maximize reconstruction accuracy. Agents are trained using a publicly available dataset which contains drives of a car equipped with Velodyne lidar \cite{geiger2013}. Theoretical background of RL is discussed in first part of this thesis. In the second part are methods from first part used to solve the Lidar-gym environment \cite{rozsypalek2018}.
  