\section{Introduction}

Lidar sensors offer an accurate distance measurement, which can be used for mapping surrounding space. There is much utilization of volumetric space reconstructions in different fields. For example, the lidar sensors are nowadays essential equipment for a large variety of autonomous vehicles. The sensor can help autonomous vehicles to orient itself in an environment. One of the most significant issues which prevent a broader implementation of these sensors is a relatively high price. Breakthrough in this field is a solid-state lidar. These lidars do not have moving parts, and their price should be circa hundreds of dollars \cite{quanergy2016}. Solid-state lidar can send a limited number of rays in chosen directions per timestamp. Zimmermann et al. \cite{zimmermann2017} proposed a mapping agent which creates dense reconstructions from sparse measurements. They also proposed prioritized greedy planning for choosing the directions of these rays. \par The objective of this thesis is to apply reinforcement learning (RL) methods to learn planning of the rays and contribute to the methods of controlling these sensors. RL is a field of study based on concepts of behavioral psychology, especially the trial and error method, and has in recent years experienced a rapid development due to the growth of computational power and neural networks improvement. Richard Sutton has made a helpful summary of RL concepts in his book \cite{sutton2012}. One of the biggest achievements was playing Atari games by a RL agent without any prior knowledge of the environment \cite{mnih2015}. Soon after was introduced a RL agent, able to solve simple continuous problems such as balancing inverse pendulum on a cart. Today state-of-the-art methods can solve complex problems with infinite action spaces. Although these methods reach the great success, they still suffer from a lack of sample efficiency - they need for training a lot of interactions with the environment. This inefficiency makes creating an agent controlling lidar very challenging, since training large neural networks is very time-consuming. \par The agent is divided into two parts - mapping and planning. The mapping part should create the best possible reconstruction from sparse measurements, while the planning part is focused on picking rays that will maximize reconstruction accuracy. Agents are trained using a publicly available dataset which contains drives of a car equipped with Velodyne lidar \cite{geiger2013}. Theoretical background of the RL is discussed in the first part of this thesis. In the second part are methods from the first part used to solve the Lidar-gym environment \cite{rozsypalek2018}.
  