\section{Introduction}

Lidar sensors offer an accurate volumetric mapping of surrounding space. There is much utilization of volumetric space reconstructions in different fields. For example, lidar sensors are nowadays essential equipment for a large variety of autonomous vehicles. The sensor can help autonomous vehicles to orientate in an environment. One of the most significant issues which prevent broader implementation of these sensors is a relatively high price. Breakthrough in this field is solid-state lidars. These lidars do not have moving parts, and their price should be circa hundreds of dollars \cite{quanergy2016}. The major drawback of this sensor is a limited number of rays which can be sent over the certain time frame and only in chosen directions. Zimmermann et al. \cite{zimmermann2017} proposed mapping agent which creates dense reconstructions from sparse measurements. They also proposed prioritized greedy planning for choosing directions of these rays. The objective of this thesis is to apply reinforcement learning (RL) methods to learn planning of the rays and contribute to methods of controlling these sensors. RL is a field of study based on concepts of behavioral psychology, especially the trial and error method, and has in recent years experienced a rapid development due to the growth of computational power and neural networks improvement. Richard Sutton has made a helpful summary of RL concepts in his book \cite{sutton2012}. One of the biggest achievements was playing Atari games by a RL agent without any prior knowledge of the environment \cite{mnih2015}. Soon after the RL agent, able to solve simple continuous problems such as balancing inverse pendulum on a cart, was introduced. Today state-of-the-art methods can solve complex environments with infinite action spaces. Although these methods reach great successes, they still suffer from lack of sample efficiency - they need for training many episodes before environment can be solved. This inefficiency makes creating agent controlling lidar very challenging, since training large neural networks is very time-consuming. The agent is divided into two parts - mapping and planning. The mapping part should create a best possible reconstruction from sparse measurements, while the planning part is focused on picking rays that will maximize reconstruction accuracy. Agents are trained using a publicly available dataset which contains drives of a car equipped with Velodyne lidar \cite{geiger2013}. Theoretical background of RL is discussed in first part of this thesis. In the second part are methods from first part used to solve the Lidar-gym environment \cite{rozsypalek2018}.
  